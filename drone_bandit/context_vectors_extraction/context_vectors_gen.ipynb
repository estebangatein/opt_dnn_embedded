{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from simple_model import simple_model\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_to_dict(model, input_size, batch_size=1, device=\"cpu\"):\n",
    "    # function to convert the summary output to a dictionary\n",
    "    model.to(device)\n",
    "    x = torch.rand(batch_size, *input_size).to(device)\n",
    "\n",
    "    summary_dict = {}\n",
    "    layer_counts = {}\n",
    "\n",
    "    def register_hook(module):\n",
    "        def hook(module, input, output):\n",
    "            module_name = module.__class__.__name__\n",
    "            # Contar capas con el mismo nombre\n",
    "            if module_name not in layer_counts:\n",
    "                layer_counts[module_name] = 0\n",
    "            layer_counts[module_name] += 1\n",
    "            name = f\"{module_name}_{layer_counts[module_name]}\"\n",
    "\n",
    "            # Verificar si la salida es un tensor o una tupla\n",
    "            if isinstance(output, tuple):\n",
    "                output_shapes = [list(o.shape) for o in output]\n",
    "            else:\n",
    "                output_shapes = list(output.shape)\n",
    "\n",
    "            # Guardar el tamaño de salida y los parámetros en el diccionario\n",
    "            summary_dict[name] = {\n",
    "                \"output_shape\": output_shapes,\n",
    "                \"params\": sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "            }\n",
    "\n",
    "        # Registrar el hook en todas las capas, excluyendo contenedores\n",
    "        if not isinstance(module, nn.ModuleList) and not isinstance(module, nn.Sequential):\n",
    "            module.register_forward_hook(hook)\n",
    "\n",
    "    model.apply(register_hook)\n",
    "    model(x)\n",
    "    \n",
    "    return summary_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_model()\n",
    "input_size = (1, 200, 200)\n",
    "summary_model = summary_to_dict(model, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Conv2d_1': {'output_shape': [1, 16, 100, 100], 'params': 416},\n",
       " 'ReLU_1': {'output_shape': [1, 16, 100, 100], 'params': 0},\n",
       " 'Conv2d_2': {'output_shape': [1, 32, 50, 50], 'params': 4640},\n",
       " 'ReLU_2': {'output_shape': [1, 32, 50, 50], 'params': 0},\n",
       " 'Linear_1': {'output_shape': [1, 4], 'params': 320004},\n",
       " 'Conv2d_3': {'output_shape': [1, 64, 25, 25], 'params': 18496},\n",
       " 'ReLU_3': {'output_shape': [1, 64, 25, 25], 'params': 0},\n",
       " 'Conv2d_4': {'output_shape': [1, 64, 13, 13], 'params': 36928},\n",
       " 'ReLU_4': {'output_shape': [1, 64, 13, 13], 'params': 0},\n",
       " 'Linear_2': {'output_shape': [1, 4], 'params': 43268},\n",
       " 'simple_model_1': {'output_shape': [[1, 4], [1, 4]], 'params': 423752}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_vectors(model, summary_dict, input_size, n_classes_ee):\n",
    "    context_vectors = []\n",
    "    previous_shape = input_size # previous shape of layer considered starts with input shape\n",
    "    model = [item for j in model.children() for item in j.children()] # serialize the model\n",
    "    intermediate_size = input_size[0] * input_size[1] * input_size[2] # assuming input is an image\n",
    "\n",
    "    for p in range(len(summary_dict)-1): # last one is the total model, so we don't need it\n",
    "        macs_conv, macs_lin, macs_act, n_conv, n_lin, n_act = 0, 0, 0, 0, 0, 0\n",
    "        model_considered = {k: v for k, v in summary_dict.items() if k in list(summary_dict.keys())[p:]} # get the layers from the current layer to the end    \n",
    "        \n",
    "        for ix, layer in enumerate(model_considered):\n",
    "\n",
    "            if 'conv' in layer.lower():\n",
    "                n_conv += 1\n",
    "                output_shape = tuple(summary_dict[layer]['output_shape'][1:]) # remove batch size\n",
    "                macs_conv += output_shape[0] * output_shape[1] * output_shape[2] * model[ix].kernel_size[0] * model[ix].kernel_size[1] * previous_shape[0]\n",
    "                # so output channels * output height * output width * kernel height * kernel width * kernel depth\n",
    "                previous_shape = output_shape\n",
    "\n",
    "            elif 'linear' in layer.lower():\n",
    "                n_lin += 1\n",
    "                output_shape = tuple(summary_dict[layer]['output_shape'][1:])\n",
    "                macs_lin += model[ix].in_features * model[ix].out_features\n",
    "                previous_shape = output_shape\n",
    "\n",
    "            elif 'relu' in layer.lower():\n",
    "                n_act += 1\n",
    "                output_shape = summary_dict[layer]['output_shape'][1:]\n",
    "                \n",
    "                total_act = 1\n",
    "                for dim in output_shape:\n",
    "                    total_act *= dim\n",
    "                macs_act += total_act\n",
    "                previous_shape =  tuple(output_shape)\n",
    "        \n",
    "        model = model[1:] # remove the current layer\n",
    "        context_vectors.append([macs_conv, macs_lin, macs_act, n_conv, n_lin, n_act, intermediate_size])\n",
    "\n",
    "        # calculate the next intermediate size considering early exits \n",
    "        # PROBLEM: MAY NOT BE ROBUST IF THE MODEL HAS MORE THAN ONE OUTPUT AND EACH OF THEM (or at least one) HAVE MORE THAN ONE LINEAL LAYER\n",
    "        if n_classes_ee not in model_considered[list(model_considered.keys())[0]]['output_shape'][1:]: \n",
    "            intermediate_size  = 1\n",
    "            for dim_out in model_considered[list(model_considered.keys())[0]]['output_shape'][1:]:\n",
    "                intermediate_size *= dim_out\n",
    "        \n",
    "        previous_shape = tuple(model_considered[list(model_considered.keys())[0]]['output_shape'][1:]) # reset previous shape to the first layer considered\n",
    "\n",
    "    context_vectors.append([0,0,0,0,0,0,0])\n",
    "    return context_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[23190016, 363264, 290816, 4, 2, 4, 40000],\n",
       " [19190016, 363264, 290816, 3, 2, 4, 160000],\n",
       " [19190016, 363264, 130816, 3, 2, 3, 160000],\n",
       " [7670016, 363264, 130816, 2, 2, 3, 80000],\n",
       " [7670016, 363264, 50816, 2, 2, 2, 80000],\n",
       " [7670016, 43264, 50816, 2, 1, 2, 80000],\n",
       " [6230016, 43264, 50816, 1, 1, 2, 40000],\n",
       " [6230016, 43264, 10816, 1, 1, 1, 40000],\n",
       " [0, 43264, 10816, 0, 1, 1, 10816],\n",
       " [0, 43264, 0, 0, 1, 0, 10816],\n",
       " [0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_context_vectors(model, summary_model, input_size, n_classes_ee=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2),  # Conv1\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),                # Pool1\n",
    "            \n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1),            # Conv2\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),                # Pool2\n",
    "            \n",
    "            nn.Conv2d(16, 120, kernel_size=5, stride=1),          # Conv3\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(480, 84),                                   # FC1\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, num_classes)                           # FC2\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5()\n",
    "input_size = (1, 32, 32)\n",
    "summary_model = summary_to_dict(model, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[691200, 41160, 9012, 3, 2, 4, 1024],\n",
       " [537600, 41160, 9012, 2, 2, 4, 6144],\n",
       " [537600, 41160, 2868, 2, 2, 3, 6144],\n",
       " [537600, 41160, 2868, 2, 2, 3, 1536],\n",
       " [192000, 41160, 2868, 1, 2, 3, 2304],\n",
       " [192000, 41160, 564, 1, 2, 2, 2304],\n",
       " [192000, 41160, 564, 1, 2, 2, 576],\n",
       " [0, 41160, 564, 0, 2, 2, 480],\n",
       " [0, 41160, 84, 0, 2, 1, 480],\n",
       " [0, 840, 84, 0, 1, 1, 84],\n",
       " [0, 840, 0, 0, 1, 0, 84],\n",
       " [0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_context_vectors(model, summary_model, input_size, n_classes_ee=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNet()\n",
    "input_size = (3, 224, 224)\n",
    "summary_model = summary_to_dict(model, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[655566528, 58621952, 493184, 5, 3, 7, 150528],\n",
       " [585289728, 58621952, 493184, 4, 3, 7, 193600],\n",
       " [585289728, 58621952, 299584, 4, 3, 6, 193600],\n",
       " [585289728, 58621952, 299584, 4, 3, 6, 46656],\n",
       " [361340928, 58621952, 299584, 3, 3, 6, 139968],\n",
       " [361340928, 58621952, 159616, 3, 3, 5, 139968],\n",
       " [361340928, 58621952, 159616, 3, 3, 5, 32448],\n",
       " [249200640, 58621952, 159616, 2, 3, 5, 64896],\n",
       " [249200640, 58621952, 94720, 2, 3, 4, 64896],\n",
       " [99680256, 58621952, 94720, 1, 3, 4, 43264],\n",
       " [99680256, 58621952, 51456, 1, 3, 3, 43264],\n",
       " [0, 58621952, 51456, 0, 3, 3, 43264],\n",
       " [0, 58621952, 8192, 0, 3, 2, 43264],\n",
       " [0, 58621952, 8192, 0, 3, 2, 9216],\n",
       " [0, 58621952, 8192, 0, 3, 2, 9216],\n",
       " [0, 20873216, 8192, 0, 2, 2, 4096],\n",
       " [0, 20873216, 4096, 0, 2, 1, 4096],\n",
       " [0, 20873216, 4096, 0, 2, 1, 4096],\n",
       " [0, 4096000, 4096, 0, 1, 1, 4096],\n",
       " [0, 4096000, 0, 0, 1, 0, 4096],\n",
       " [0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_context_vectors(model, summary_model, input_size, n_classes_ee=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
